{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab1: Efficient AI Fundamentals\n",
    "\n",
    "In this lab, you will learn the foundational concepts of efficient AI through hands-on analysis of an LLM decoder layer.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Calculate FLOPs and memory access** for transformer operations\n",
    "2. **Build and interpret roofline models** to identify compute vs memory bottlenecks\n",
    "3. **Profile GPU kernels** using PyTorch Profiler\n",
    "4. **Understand kernel launch overhead** and optimization techniques (CUDA Graphs, torch.compile)\n",
    "5. **Analyze Flash Attention** and explain why it's faster\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- PyTorch 2.0+\n",
    "- CUDA-capable GPU\n",
    "- Transformers library\n",
    "- Flash Attention 2 (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Our utility functions\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "from utils import *\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1: Fundamental Metrics — Latency, MAC, and I/O\n",
    "\n",
    "In this section, we will learn three fundamental metrics: latency, MAC, and I/O. We will start with the most common operator, matrix/vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 1.1\n",
    "\n",
    "Please write a tri-loop version of matrix multiplication (please don't use any libraries such as Numpy, Scipy, and PyTorch API for this question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gemm(A, B):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A (torch.Tensor): Input matrix shaped (N, K) on any device/dtype.\n",
    "        B (torch.Tensor): Input matrix shaped (K, M) on the same device/dtype as A.\n",
    "    Returns:\n",
    "        torch.Tensor: Product matrix C with shape (N, M) stored on A/B's device and dtype.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1 Latency\n",
    "\n",
    "As simple as it means, latency refers to how long does your program take to excute on your device. It is a basic yet important metric for efficiency evaluation. Below is a standard way to measure latency on GPU. Below is code for evaluating a function execution one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def latency_one_time(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fn (Callable): Target function to be timed once.\n",
    "        *args: Positional arguments forwarded to fn.\n",
    "        **kwargs: Keyword arguments forwarded to fn.\n",
    "    Returns:\n",
    "        None: Timing side effect only; elapsed time is stored in a local variable.\n",
    "    \"\"\"\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    # Start recording\n",
    "    start.record()\n",
    "    # Do your stuff here\n",
    "    fn(*args, **kwargs)\n",
    "    # End recording\n",
    "    end.record()\n",
    "    # Call torch.cuda.synchronize() to wait for all the previous operations to finish\n",
    "    torch.cuda.synchronize()\n",
    "    # Get the elapsed time\n",
    "    elapsed_time = start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 1.2\n",
    "\n",
    "Please measure the latency of your `gemm` multiple times. Describe your findings.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on your findings, you will realize that evaluating the function one time is highly unreliable. Therefore, the standard way to measure latency is to warmup the program then measure the average latency of multiple runs. The code is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def latency(fn, *args, warmup = 10, repeat = 100, **kwargs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fn (Callable): Target function to be benchmarked.\n",
    "        *args: Positional inputs forwarded to fn.\n",
    "        warmup (int): Number of warmup invocations to stabilize kernels.\n",
    "        repeat (int): Number of timed iterations to average.\n",
    "        **kwargs: Keyword inputs forwarded to fn.\n",
    "    Returns:\n",
    "        float: Mean latency in seconds computed from CUDA event timings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Warmup the function for the number of 'warmup' times. Make sure to call `torch.cuda.synchronize()` at the end of warmup.\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        fn(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Measure the time of each excution for the number of 'repeat' times. Record the time of each excution in 'times' list.\n",
    "    times = []\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        fn(*args, **kwargs)\n",
    "        end.record()\n",
    "        # Don't forget to synchronize the device\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "    # Return the average time of the repeated excutions in second\n",
    "    times = torch.tensor(times)\n",
    "    return times.mean().item() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's see the latency of matrix-matrix and matrix-vector multiplication (this could take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(64, 128, dtype=torch.float32, device=device)\n",
    "b = torch.randn(128, 64, dtype=torch.float32, device=device)\n",
    "v = torch.randn(128, 1, dtype=torch.float32, device=device)\n",
    "gemm_latency = latency(gemm, a, b, warmup=3, repeat=10)\n",
    "gemv_latency = latency(gemm, a, v, warmup=3, repeat=10)\n",
    "print(\"gemm_latency\", gemm_latency)\n",
    "print(\"gemv_latency\", gemv_latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, the most naive matrix multiplication is very slow for large matrices. This is because the tri-loop is highly inefficient and doesn't make use of the modern hardware. Let's now try to optimize a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 1.3\n",
    "\n",
    "As you have learned in Linear Algebra, the matrix multiplication can also be computed in a tiled way: $ C^{(t_i, t_j)} = A^{(t_i, :)} \\times B^{(:, t_j)} $. A disgram for demonstration is as follows:\n",
    "\n",
    "![alt text](./assets/tiled_matmul.png)\n",
    "\n",
    "Please implement the `gemm_tiled` in the cell below according to the formula. To multiply two tiles, please use `@` operator. Then, measure the latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gemm_tiled(A, B, block_size_n: int = 16, block_size_m: int = 16):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A (torch.Tensor): Left matrix shaped (N, K) to be multiplied.\n",
    "        B (torch.Tensor): Right matrix shaped (K, M) to be multiplied.\n",
    "        block_size_n (int): Tile size along the N dimension.\n",
    "        block_size_m (int): Tile size along the M dimension.\n",
    "    Returns:\n",
    "        torch.Tensor: Blockwise product with shape (N, M) matching A/B device and dtype.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Measure the latency of gemm_vec\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you are familiar with parallel programming, you should realize that the tiled version can be eaisly parallelized. In fact, there's still a lot of room for improvement. For example, `torch.matmul` is a highly optimized for matrix multiplication. You can refer to resources such as [NVIDIA CUTLASS](https://github.com/NVIDIA/cutlass) and [Triton matmul kernel](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html) if you are interested in how matmul is optmized on modern GPUs. Please run the cell below to feel the power of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sizes = [512, 1024, 2048, 4096]\n",
    "gemm_tiled_latencies = []\n",
    "torch_matmul_latencies = []\n",
    "for size in sizes:\n",
    "    a = torch.randn(size, size, dtype=torch.float32, device=device)\n",
    "    b = torch.randn(size, size, dtype=torch.float32, device=device)\n",
    "    gemm_tiled_latencies.append(latency(gemm_tiled, a, b, warmup=3, repeat=10))\n",
    "    torch_matmul_latencies.append(latency(torch.matmul, a, b, warmup=3, repeat=10))\n",
    "\n",
    "    del a, b\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(gemm_tiled_latencies)\n",
    "print(torch_matmul_latencies)\n",
    "plot_gemm_torch_matmul_latency(sizes, gemm_tiled_latencies, torch_matmul_latencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1.2: MAC & FLOPs\n",
    "\n",
    "**Multiply-Accumulate (MAC)** operation is a fundamental building block in digital signal processing and artificial intelligence hardware. It computes the product of two numbers and adds that result to an accumulating sum, effectively performing the operation $a \\leftarrow a + (b \\times c)$. This single-cycle instruction is critical because it forms the basis of dot products and matrix multiplications, which are the primary mathematical engines behind neural networks.\n",
    "\n",
    "**FLOPs (Floating Point Operations)** count the total number of individual arithmetic operations involving decimal numbers. These operations include addition, subtraction, multiplication, and division.\n",
    "\n",
    "Based on the above definitions, you can see that 1 MAC consists of 2 FLOPs (1 for multiplication and 1 for addition).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 1.2\n",
    "\n",
    "Please calculate the MAC of your matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gemm_mac(x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (torch.Tensor): Left matrix shaped (N, K).\n",
    "        y (torch.Tensor): Right matrix shaped (K, M).\n",
    "    Returns:\n",
    "        int: Multiply-accumulate count for computing x @ y.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Public Testcase\n",
    "A = torch.randn(512, 512, dtype=torch.float32, device=device)\n",
    "B = torch.randn(512, 512, dtype=torch.float32, device=device)\n",
    "assert gemm_mac(A, B) == 134217728\n",
    "\n",
    "A = torch.randn(512, 256, dtype=torch.float32, device=device)\n",
    "B = torch.randn(256, 512, dtype=torch.float32, device=device)\n",
    "assert gemm_mac(A, B) == 67108864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1.3: I/O\n",
    "\n",
    "I/O, which is memory access, consists of the following parts:\n",
    "- **Read**: Loading weights and input activations from HBM\n",
    "- **Write**: Storing output activations to HBM\n",
    "Thus, the I/O cost of one operation is simply the total memory bytes read plus the total memory bytes written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 1.3\n",
    "\n",
    "Finish the code below to compute the I/O of the `gemm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gemm_io(A, B):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A (torch.Tensor): Left matrix shaped (N, K).\n",
    "        B (torch.Tensor): Right matrix shaped (K, M).\n",
    "    Returns:\n",
    "        int: Total bytes read/written (assuming fp32) for computing A @ B.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Public Testcase\n",
    "A = torch.randn(512, 512, dtype=torch.float32, device=device)\n",
    "B = torch.randn(512, 512, dtype=torch.float32, device=device)\n",
    "assert gemm_io(A, B) == 3145728\n",
    "\n",
    "A = torch.randn(512, 256, dtype=torch.float32, device=device)\n",
    "B = torch.randn(256, 512, dtype=torch.float32, device=device)\n",
    "assert gemm_io(A, B) == 2097152\n",
    "\n",
    "A = torch.randn(128, 256, dtype=torch.float32, device=device)\n",
    "B = torch.randn(256, 512, dtype=torch.float32, device=device)\n",
    "assert gemm_io(A, B) == 917504\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 2: Roofline Model\n",
    "\n",
    "When we run algorithms on hardware, we're bounded by three things: how fast our computer can do math (OPs/second), the bandwidth available for moving data around (bytes/second), and the total memory available to store data (bytes). These “roofline” constraints let us upper and lower bound the time of a given computation.\n",
    "\n",
    "**Arthemetic Intensity:** The arithmetic intensity of an algorithm is given by the ratio of the total FLOPs it performs to the number of bytes it needs to communicate — either within a chip or between chips.\n",
    "\n",
    "$$ \\text{Arthemetic Intensity (AI)} = \\frac{\\text{Communication FLOPs}}{\\text{Communication Bytes}} $$\n",
    "\n",
    "In Roofline model, we define the ridge point as the arithmetic intensity where the roofline transitions from memory-bound to compute-bound, calculated as $ \\frac{\\text{peak\\_FLOPs}}{\\text{peak\\_memory\\_bandwidth}}$. Different GPU devices have different ridge points.\n",
    "\n",
    "The roofline has two regions:\n",
    "- **Memory-bound region** (left of ridge point): Performance = Bandwidth × AI\n",
    "- **Compute-bound region** (right of ridge point): Performance = Peak FLOPs\n",
    "\n",
    "![alt text](./assets/roofline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 2.1\n",
    "\n",
    "Plot the roofline of matrix-matrix multiplication of the given dimensions. You can use the `plot_roofline` function we provide for you. You can assume all matrices are in `float32`. Describe you findings based on the roofline diagram.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Ns = [1024, 2048, 4096, 8192]\n",
    "gpu_name = \"A100-80GB\"\n",
    "gpu_spec = GPU_SPECS[gpu_name]\n",
    "peak_flops = gpu_spec[\"fp32_tflops\"]\n",
    "peak_bandwidth = gpu_spec[\"memory_bandwidth_tb_s\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 2.2\n",
    "\n",
    "Now let's plot the roofline of gemv. The settings are the same as above. Describe your findings.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Ns = [1024, 2048, 4096, 8192]\n",
    "gpu_name = \"A100-80GB\"\n",
    "gpu_spec = GPU_SPECS[gpu_name]\n",
    "peak_flops = gpu_spec[\"fp32_tflops\"]\n",
    "peak_bandwidth = gpu_spec[\"memory_bandwidth_tb_s\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 3: Case Study - Gemma-3 Decoder Layer\n",
    "\n",
    "In this section, we will walk you through [gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it), a light weight instruction-tuned language model, as a case study to enhance your profiling knowledge.\n",
    "\n",
    "All components of the model's one decoder layer are shown below. These components are (in the order of forward excution)\n",
    "- **Input LayerNorm**\n",
    "- **Self-Attention Layer**\n",
    "    - Query and Key Norm\n",
    "    - Query, Key, and Value Projections\n",
    "    - Rotary Embedding\n",
    "    - Outout Projection\n",
    "- **Post Attention LayerNorm**\n",
    "- **Pre MLP LayerNorm**\n",
    "- **MLP**\n",
    "    - Gate Projection\n",
    "    - Up Projection\n",
    "    - Activation\n",
    "    - Down Projection\n",
    "- **Post MLP LayerNorm**\n",
    "\n",
    "You don't have to know all details. We will break it down later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.utils import TransformersKwargs\n",
    "from transformers.utils.generic import maybe_autocast\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
    "\n",
    "\n",
    "class GELUTanh(nn.Module):\n",
    "\n",
    "    def __init__(self, use_gelu_tanh_python: bool = False):\n",
    "        super().__init__()\n",
    "        self.act = functools.partial(nn.functional.gelu, approximate=\"tanh\")\n",
    "\n",
    "    # def _gelu_tanh_python(self, input):\n",
    "    #     return input * 0.5 * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.act(input)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class Gemma3MLP(nn.Module):\n",
    "    def __init__(self, config: Gemma3TextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = GELUTanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.profiler.record_function(\"gate_proj\"):\n",
    "            gate = self.gate_proj(x)  \n",
    "        with torch.profiler.record_function(\"up_proj\"):\n",
    "            up_x = self.up_proj(x)\n",
    "        with torch.profiler.record_function(\"act_fn\"):\n",
    "            gate_up = self.act_fn(gate) * up_x  \n",
    "        with torch.profiler.record_function(\"down_proj\"):\n",
    "            down_proj = self.down_proj(gate_up)  \n",
    "        return down_proj\n",
    "\n",
    "\n",
    "class Gemma3RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float())\n",
    "        output = output * (1.0 + self.weight.float())\n",
    "        return output.type_as(x)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n",
    "\n",
    "\n",
    "class Gemma3Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.layer_type = config.layer_types[layer_idx] if hasattr(config, \"layer_types\") else None\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = config.query_pre_attn_scalar**-0.5\n",
    "        self.attention_dropout = self.config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "        self.attn_logit_softcapping = None\n",
    "        self.sliding_window = None\n",
    "        self.is_sliding = False\n",
    "\n",
    "        self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n",
    "        self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: torch.Tensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_kv_cache: bool = True,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        prefill_hidden_states: Optional[torch.Tensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        with torch.profiler.record_function(\"q_proj\"):\n",
    "            query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        \n",
    "        query_states = self.q_norm(query_states)\n",
    "\n",
    "        if use_kv_cache:\n",
    "            with torch.profiler.record_function(\"k_proj\"):\n",
    "                key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "            with torch.profiler.record_function(\"v_proj\"):\n",
    "                value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "            key_states = self.k_norm(key_states)\n",
    "            cos, sin = position_embeddings\n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "            assert past_key_values is not None, \"past_key_values should not be None when use_kv_cache is True\"\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_cache, value_cache = past_key_values.layers[self.layer_idx].keys, past_key_values.layers[self.layer_idx].values\n",
    "            if key_cache is not None:\n",
    "                key_states = torch.cat([key_cache, key_states], dim=2)\n",
    "                value_states = torch.cat([value_cache, value_states], dim=2)\n",
    "            else:\n",
    "                past_key_values.layers[self.layer_idx].keys = key_states\n",
    "                past_key_values.layers[self.layer_idx].values = value_states\n",
    "        \n",
    "        else:\n",
    "            assert prefill_hidden_states is not None, \"prefill_hidden_states should not be None when use_kv_cache is False\"\n",
    "            all_hidden_states = torch.cat([prefill_hidden_states, hidden_states], dim=1)\n",
    "            full_shape = (*all_hidden_states.shape[:-1], -1, self.head_dim)\n",
    "            key_states = self.k_proj(all_hidden_states).view(full_shape).transpose(1, 2)\n",
    "            value_states = self.v_proj(all_hidden_states).view(full_shape).transpose(1, 2)\n",
    "            key_states = self.k_norm(key_states)\n",
    "            cos, sin = position_embeddings\n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # we will use flash attention by default\n",
    "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        with torch.profiler.record_function(\"attn_op\"):\n",
    "            attn_output, _ = attention_interface(\n",
    "                self,\n",
    "                query_states,\n",
    "                key_states,\n",
    "                value_states,\n",
    "                attention_mask,\n",
    "                dropout=self.attention_dropout if self.training else 0.0,\n",
    "                scaling=self.scaling,\n",
    "                sliding_window=self.sliding_window,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class Gemma3RotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor\n",
    "\n",
    "    def __init__(self, config, device=None, layer_type=None):\n",
    "        super().__init__()\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_type = {}\n",
    "        rope_params = self.config.rope_parameters[layer_type]\n",
    "        self.rope_type[layer_type] = rope_params[\"rope_type\"]\n",
    "        rope_init_fn = self.compute_default_rope_parameters\n",
    "        curr_inv_freq, curr_attention_scaling = rope_init_fn(self.config, device, layer_type=layer_type)\n",
    "        self.register_buffer(f\"{layer_type}_inv_freq\", curr_inv_freq, persistent=False)\n",
    "        setattr(self, f\"{layer_type}_original_inv_freq\", curr_inv_freq)\n",
    "        setattr(self, f\"{layer_type}_attention_scaling\", curr_attention_scaling)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_default_rope_parameters(\n",
    "        config: Optional[Gemma3TextConfig] = None,\n",
    "        device: Optional[\"torch.device\"] = None,\n",
    "        seq_len: Optional[int] = None,\n",
    "        layer_type: Optional[str] = None,\n",
    "    ) -> tuple[\"torch.Tensor\", float]:\n",
    "        base = config.rope_parameters[layer_type][\"rope_theta\"]\n",
    "        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
    "\n",
    "        attention_factor = 1.0\n",
    "        inv_freq = 1.0 / (\n",
    "            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n",
    "        )\n",
    "        return inv_freq, attention_factor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids, layer_type=None):\n",
    "        inv_freq = getattr(self, f\"{layer_type}_inv_freq\")\n",
    "        attention_scaling = getattr(self, f\"{layer_type}_attention_scaling\")\n",
    "\n",
    "        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        with maybe_autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos() * attention_scaling\n",
    "            sin = emb.sin() * attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "class Gemma3DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.layer_idx = layer_idx\n",
    "        self.attention_type = \"full_attention\"\n",
    "        self.self_attn = Gemma3Attention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = Gemma3MLP(config)\n",
    "        self.input_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: torch.Tensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        with torch.profiler.record_function(\"input_norm\"):\n",
    "            hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        with torch.profiler.record_function(\"self_attn\"):\n",
    "            hidden_states = self.self_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                cache_position=cache_position,\n",
    "                **kwargs,\n",
    "            )\n",
    "        \n",
    "        with torch.profiler.record_function(\"post_attn_norm\"):\n",
    "            hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        with torch.profiler.record_function(\"res\"):\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        with torch.profiler.record_function(\"pre_ffn_norm\"):\n",
    "            hidden_states = self.pre_feedforward_layernorm(hidden_states)\n",
    "        with torch.profiler.record_function(\"mlp\"):\n",
    "            hidden_states = self.mlp(hidden_states)  \n",
    "        with torch.profiler.record_function(\"post_ffn_norm\"):\n",
    "            hidden_states = self.post_feedforward_layernorm(hidden_states)\n",
    "        with torch.profiler.record_function(\"res\"):\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3.1: MAC & FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GELUTanh(nn.Module):\n",
    "    def __init__(self, use_gelu_tanh_python: bool = False):\n",
    "        super().__init__()\n",
    "        self.act = functools.partial(nn.functional.gelu, approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.act(input)\n",
    "\n",
    "class Gemma3MLP(nn.Module):\n",
    "    def __init__(self, config: Gemma3TextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = GELUTanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)  \n",
    "        up_x = self.up_proj(x)\n",
    "        gate_up = self.act_fn(gate) * up_x  \n",
    "        down_proj = self.down_proj(gate_up)  \n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section, we will walk through the MAC & FLOPs computation of the MLP in the decoder layer. The MLP takes in a tensor `x` of shape `(B, L, D)` and outputs a tensor of the same shape. Based on the code below, we can break down the operations as follows:\n",
    "- `gate_proj`: This is a linear layer with weight $W \\in \\mathbb{R}^{I \\times D}$, so\n",
    "    - MAC = $B \\times L \\times D \\times I$\n",
    "    - FLOPs = $2 \\times B \\times L \\times D \\times I$\n",
    "- `up_proj`: This is a linear layer with weight $W \\in \\mathbb{R}^{I \\times D}$, so\n",
    "    - MAC = $B \\times L \\times D \\times I$\n",
    "    - FLOPs = $2 \\times B \\times L \\times D \\times I$\n",
    "- `act_fn`: The input is a tensor with shape `(B, L, I)`. Each element goes through 8 operations (6 mul, 2 add, and note that we consider non-fused GeLU here which will be discussed later). So, the\n",
    "    - MAC is not measurable (numerically 0) because a GeLU unit can't be simplified into a single clock-cycle MAC.\n",
    "    - FLOPs = $8 \\times B \\times L \\times I$\n",
    "- `gate_up`: This is a element-wise matmul of two matrices of shape `(B, L, I)`, so\n",
    "    - MAC is also not applicable in this case (numerically 0) since there's no \"add\"\n",
    "    - FLOPs = $B \\times L \\times I$\n",
    "- `down_proj`: This is a linear layer with weight $W \\in \\mathbb{R}^{D \\times I}$, so \n",
    "    - MAC = $B \\times L \\times I \\times D$\n",
    "    - FLOPs = $2 \\times B \\times L \\times I \\times D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_mlp_mac(B, L, D, I):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        B (int): Batch size.\n",
    "        L (int): Sequence length.\n",
    "        D (int): Hidden dimension.\n",
    "        I (int): Intermediate (FFN) dimension.\n",
    "    Returns:\n",
    "        int: Total MACs for the MLP block (gate, up, down projections).\n",
    "    \"\"\"\n",
    "    gate_proj = B * L * D * I\n",
    "    up_proj = B * L * D * I\n",
    "    act_fn = 0\n",
    "    gate_up = 0\n",
    "    down_proj = B * L * I * D\n",
    "    return gate_proj + up_proj + act_fn + gate_up + down_proj\n",
    "\n",
    "def calc_mlp_flops(B, L, D, I):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        B (int): Batch size.\n",
    "        L (int): Sequence length.\n",
    "        D (int): Hidden dimension.\n",
    "        I (int): Intermediate (FFN) dimension.\n",
    "    Returns:\n",
    "        int: Total floating-point operations for the full MLP path.\n",
    "    \"\"\"\n",
    "    gate_proj = 2 * B * L * D * I\n",
    "    up_proj = 2 * B * L * D * I\n",
    "    act_fn = 8 * B * L * I\n",
    "    gate_up = B * L * I\n",
    "    down_proj = 2 * B * L * I * D\n",
    "    return gate_proj + up_proj + act_fn + gate_up + down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 3.1\n",
    "\n",
    "(1) Please implement the standard scale-dot-product attention from scratch. You can assume all tensors are in `float32`. (2) Then, calculate its MAC. The shape of q is `(B, H, L_q, D)` and the shape of k,v is `(B, H, L_kv, D)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, attn_mask=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q (torch.Tensor): Query tensor of shape (B, H, L_q, D_h).\n",
    "        k (torch.Tensor): Key tensor of shape (B, H, L_kv, D_h).\n",
    "        v (torch.Tensor): Value tensor of shape (B, H, L_kv, D_h).\n",
    "        attn_mask (torch.Tensor, optional): Attention mask of shape (B, H, L_q, L_kv).\n",
    "    Returns:\n",
    "        torch.Tensor: Attention output of shape (B, H, L_q, D_h).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def calc_attn_mac(B, H, L_q, L_kv, D_h):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        B (int): Batch size.\n",
    "        H (int): Number of attention heads.\n",
    "        L_q (int): Query sequence length.\n",
    "        L_kv (int): Key/value sequence length.\n",
    "        D_h (int): Per-head hidden size.\n",
    "    Returns:\n",
    "        int: Total MACs for scaled dot-product attention.\n",
    "    \"\"\"\n",
    "    # calculate the MAC of q @ k.T\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # MAC is not applicable to softmax, so we ignore it\n",
    "    softmax = 0\n",
    "\n",
    "    # calculate the MAC of attn @ v\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return qk + softmax + attn_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Public Testcase for SDPA\n",
    "q = torch.randn(1, 4, 1024, 128, dtype=torch.float32, device=device)\n",
    "k = torch.randn(1, 4, 1024, 128, dtype=torch.float32, device=device)\n",
    "v = torch.randn(1, 4, 1024, 128, dtype=torch.float32, device=device)\n",
    "out = scaled_dot_product_attention(q, k, v)\n",
    "expected = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "torch.testing.assert_close(out, expected)\n",
    "\n",
    "# Public Testcase for MAC\n",
    "B, H, L_q, L_kv, D_h = 1, 4, 173, 256, 128\n",
    "assert calc_attn_mac(B, H, L_q, L_kv, D_h) == 45350912\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bonus 3.1\n",
    "\n",
    "Can you compute the MAC of the whole `GemmaAttention` module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_attn_module_mac(batch_size, query_len, kv_len, gemma_config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch_size (int): Batch size.\n",
    "        query_len (int): Query sequence length.\n",
    "        kv_len (int): Key/value sequence length.\n",
    "        gemma_config (GemmaConfig): Gemma configuration.\n",
    "    Returns:\n",
    "        int: Total MACs for the whole `GemmaAttention` module.\n",
    "    \"\"\"\n",
    "\n",
    "    B = batch_size\n",
    "    S_q = query_len\n",
    "    S_kv = kv_len\n",
    "    H = gemma_config.hidden_size\n",
    "    n_h = gemma_config.num_attention_heads\n",
    "    n_kv = gemma_config.num_key_value_heads\n",
    "    d = gemma_config.head_dim\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3.2: I/O\n",
    "\n",
    "Next, let's practice the I/O calculation. Again, let's first analyze the MLP. Do note that we assume model weights are already loaded and exclude I/O of model weight loading from our computation for simplicity. We again breakdown the layer as follows:\n",
    "- `gate_proj`: Read tensors of shapes `(B, L, D)` and `(I, D)`, and write a tensor of shape `(B, L, I)`. So the I/O is $2 \\times (B \\times L \\times D + D \\times I + B \\times L \\times I)$\n",
    "- `up_x`: Read tensors of shapes `(B, L, D)` and `(I, D)`, and write a tensor of shape `(B, L, I)`. So the I/O is $2 \\times (B \\times L \\times D + D \\times I + B \\times L \\times I)$\n",
    "- `act_fn`: Read a tensor of shape `(B, L, I)` and write a tensor of the same shape. So the I/O is $2 \\times 2 \\times B \\times L \\times I$\n",
    "- `gate_up`: Read two tensors of shape `(B, L, I)` and write a tensor of `(B, L, I)`. SO the I/O is $2 \\times 3 \\times B \\times L \\times I$\n",
    "- `down_proj`: Read tensors of shapes `(B, L, I)` and `(D, I)`, and write a tensor of shape `(B, L, D)`. So the I/O is $2 \\times (B \\times L \\times I + D \\times I + B \\times L \\times D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_mlp_io(B, L, D, I):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        B (int): Batch size.\n",
    "        L (int): Sequence length.\n",
    "        D (int): Hidden dimension.\n",
    "        I (int): Intermediate (FFN) dimension.\n",
    "    Returns:\n",
    "        int: Total bytes read/written for the MLP block assuming fp16/bf16 (2 bytes per element) doubled for read/write.\n",
    "    \"\"\"\n",
    "    gate_proj = 2 * (B * L * D + D * I + B * L * I)\n",
    "    up_x = 2 * (B * L * D + D * I + B * L * I)\n",
    "    act_fn = 2 * 2 * B * L * I\n",
    "    gate_up = 2 * 3 * B * L * I\n",
    "    down_proj = 2 * (B * L * I + D * I + B * L * D)\n",
    "    return gate_proj + up_x + act_fn + gate_up + down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 3.2\n",
    "\n",
    "Please calculate the I/O of the standard attention you just implemented. The shape of q is `(B, H, L_q, D)` and the shape of k,v is `(B, H, L_kv, D)`. All tensors are in `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_attn_io(B, H, L_q, L_kv, D_h):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        B (int): Batch size.\n",
    "        H (int): Number of attention heads.\n",
    "        L_q (int): Query sequence length.\n",
    "        L_kv (int): Key/value sequence length.\n",
    "        D_h (int): Per-head hidden size.\n",
    "    Returns:\n",
    "        int: Total bytes moved (bf16, read+write) for standard attention.\n",
    "    \"\"\"\n",
    "    # Hint: for softmax, you can use the following formula:\n",
    "    # I/O(softmax) = 5 * (B * H * L_q * L_kv) * 4(fp32)\n",
    "\n",
    "    bytes_per_elem = 4\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bonus 3.2\n",
    "\n",
    "Can you compute the I/O of the whole `GemmaAttention` module? You only need to consider the 'prefill' stage and all tensors are in `bfloat16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_attn_module_io(batch_size, query_len, kv_len, gemma_config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch_size (int): Batch size.\n",
    "        query_len (int): Query sequence length.\n",
    "        kv_len (int): Key/value sequence length.\n",
    "        gemma_config (GemmaConfig): Gemma configuration.\n",
    "    Returns:\n",
    "        int: Total bytes moved (bf16, read+write) for the whole `GemmaAttention` module.\n",
    "    \"\"\"\n",
    "\n",
    "    B = batch_size\n",
    "    S_q = query_len\n",
    "    S_kv = kv_len\n",
    "    H = gemma_config.hidden_size\n",
    "    n_h = gemma_config.num_attention_heads\n",
    "    n_kv = gemma_config.num_key_value_heads\n",
    "    d = gemma_config.head_dim\n",
    "    bytes_per_elem = 2\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3.3: Latency of Prefill and Decode\n",
    "\n",
    "The inference process of current LLMs can be divided into two stages:\n",
    "- Prefill stage processes the entire input prompt and stores the KV cache (a [blog](https://huggingface.co/blog/not-lain/kv-caching) in case you are not familiar with KV cache)\n",
    "- Decode stage generates tokens one-by-one autoregressively using KV cache\n",
    "\n",
    "We provide the code for measuring latencies of prefill, decode without kv cache, and decode with kv cache in the following cells. You can see how much speedup the kv cache bring to the inference.\n",
    "\n",
    "**Please run this part on A6000 or A100. A10 doesn't have enough memory for the default configuration. You can feel free to try smaller inputs, but it's likely you don't see any improvements. There are no questions in this section, so you can skip it if you are using A10 already**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 8192\n",
    "hidden_size = GEMMA_CONFIG.hidden_size\n",
    "layer = Gemma3DecoderLayer(GEMMA_CONFIG, 5).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "rotary_emb = Gemma3RotaryEmbedding(GEMMA_CONFIG, layer_type=\"full_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prefill (after prefilling, kv cache will be stored in past_key_values)\n",
    "prefill_input, prefill_position_ids = create_prefill_inputs(batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device=\"cuda\")\n",
    "prefill_position_embeddings = get_position_embeddings(prefill_input, prefill_position_ids, rotary_emb, layer_type=\"full_attention\")\n",
    "past_key_values = initialize_kv_cache(config=GEMMA_CONFIG, max_cache_len=seq_len)\n",
    "latency(\n",
    "    prefill_forward, \n",
    "    layer=layer,\n",
    "    hidden_states=prefill_input, \n",
    "    position_ids=prefill_position_ids, \n",
    "    position_embeddings=prefill_position_embeddings,\n",
    "    past_key_values=past_key_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode without KV cache\n",
    "decode_input, decode_position_ids, prefill_hidden_states = create_decode_inputs_without_cache(batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device=\"cuda\")\n",
    "decode_position_embeddings = get_position_embeddings(decode_input, decode_position_ids, rotary_emb, layer_type=\"full_attention\")\n",
    "latency(\n",
    "    decode_forward, \n",
    "    layer=layer,\n",
    "    hidden_states=decode_input, \n",
    "    position_ids=decode_position_ids, \n",
    "    position_embeddings=decode_position_embeddings,\n",
    "    past_key_values=None,\n",
    "    cache_positions=None,\n",
    "    use_kv_cache=False,\n",
    "    prefill_hidden_states=prefill_hidden_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Decode with KV cache\n",
    "decode_input, decode_position_ids, decode_cache_positions = create_decode_inputs_with_cache(batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device=\"cuda\")\n",
    "decode_position_embeddings = get_position_embeddings(decode_input, decode_position_ids, rotary_emb, layer_type=\"full_attention\")\n",
    "latency(\n",
    "    decode_forward, \n",
    "    layer=layer,\n",
    "    hidden_states=decode_input, \n",
    "    position_ids=decode_position_ids, \n",
    "    position_embeddings=decode_position_embeddings,\n",
    "    past_key_values=past_key_values,\n",
    "    cache_positions=decode_cache_positions,\n",
    "    use_kv_cache=True,\n",
    "    prefill_hidden_states=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del prefill_input, prefill_position_ids, prefill_position_embeddings, past_key_values, decode_input, decode_position_ids, decode_position_embeddings, decode_cache_positions, prefill_hidden_states\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3.4: Roofline\n",
    "\n",
    "Next, we will plot the roofline using the FLOP and I/O analysis above. Let's first see how the prefill performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "B = 8\n",
    "D = GEMMA_CONFIG.hidden_size\n",
    "D_h = GEMMA_CONFIG.head_dim\n",
    "I = GEMMA_CONFIG.intermediate_size\n",
    "H = GEMMA_CONFIG.num_attention_heads\n",
    "L_q = 2048\n",
    "L_kv = 2048\n",
    "\n",
    "mlp_prefill_flops = calc_mlp_flops(B, L_q, D, I)\n",
    "mlp_prefill_io = calc_mlp_io(B, L_q, D, I)\n",
    "mlp_decode_flops = calc_mlp_flops(B, 1, D, I)\n",
    "mlp_decode_io = calc_mlp_io(B, 1, D, I)\n",
    "\n",
    "ai_to_plot = {\n",
    "    'mlp_prefill': mlp_prefill_flops / mlp_prefill_io,\n",
    "    'mlp_decode': mlp_decode_flops / mlp_decode_io,\n",
    "}\n",
    "\n",
    "plot_roofline(\n",
    "    gpu_name,\n",
    "    peak_flops,\n",
    "    peak_bandwidth,\n",
    "    ai_to_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can clearly see from the figure above that the bottleneck of MLP is very different on the two stages. During prefilling, the layer is compute-bound. During decoding, it becomes memory-bound. In fact, this is not restricted to MLP. It's an important feature of modern LLM whose inference can be divided into prefill and decode stages. The prefilling is usually compute-bound and decoding is usually memory bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 3.4.1\n",
    "\n",
    "Explain in words why the performance bottleneck of the MLP layer is different for prefilling and decoding. (Hint: think about the shapes of the query for prefilling and decoding, and how they are related to gemm and gemv)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 3.4.2\n",
    "\n",
    "Vary the batch size (B) of the decoding stage and plot them on the same roofline. Describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Bs = [1, 4, 16, 64]\n",
    "D = GEMMA_CONFIG.hidden_size\n",
    "D_h = GEMMA_CONFIG.head_dim\n",
    "I = GEMMA_CONFIG.intermediate_size\n",
    "H = GEMMA_CONFIG.num_attention_heads\n",
    "L_q = 1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 3.4.3\n",
    "\n",
    "Vary the prefill length (L_q) of the prefilling and plot them on the same roofline. Describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "B = 32\n",
    "D = GEMMA_CONFIG.hidden_size\n",
    "D_h = GEMMA_CONFIG.head_dim\n",
    "I = GEMMA_CONFIG.intermediate_size\n",
    "H = GEMMA_CONFIG.num_attention_heads\n",
    "Ls = [64, 256, 1024, 4096]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 4: Profiling with PyTorch Profiler\n",
    "\n",
    "PyTorch Profiler is a built-in performance analysis tool that helps you understand where time and memory are spent in your models. It captures detailed metrics including CPU and CUDA kernel execution times, memory allocations, and call stacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1 Basic Profiling\n",
    "\n",
    "Let's profile the matmul function using the Pytorch profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "A = torch.randn(128, 128, dtype=torch.bfloat16, device=device)\n",
    "B = torch.randn(128, 128, dtype=torch.bfloat16, device=device)\n",
    "with profile(schedule=default_schedule) as prof:\n",
    "    for _ in range(10):\n",
    "        y = torch.matmul(A, B)\n",
    "        prof.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see what CUDA kernels are actually being called. Name of CUDA kernel tells us something about the implementation. For example, at::native::reduce_kernel refers to the tensor reduction which could be the max/sum operation in your softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2 Profiling MLP\n",
    "\n",
    "Next, let's profile something more complex. We again use our MLP layer for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_len = 4096\n",
    "hidden_size = GEMMA_CONFIG.hidden_size\n",
    "x = torch.randn(batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device=device)\n",
    "mlp = Gemma3MLP(GEMMA_CONFIG).to(device=device, dtype=torch.bfloat16)\n",
    "\n",
    "# profile\n",
    "with profile(schedule=default_schedule) as prof:\n",
    "    for _ in range(10):\n",
    "        _ = mlp(x)\n",
    "        prof.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that there are a lot more kernels bing invoked this time. In fact, we can visualize the operation timeline in a nicer way. First, save the trace file of the profiler by `prof.export_chrome_trace(\"trace.json\")`. Then, go to chrome://tracing and upload the file. You will then see the following diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](./assets/mlp_tracing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bonus 4.2\n",
    "\n",
    "Can you profile the decoding stage of the full decoder layer and show the timeline visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![alt text](./assets/tracing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.3 Kernel Fusion\n",
    "\n",
    "As you can see from the profiling results, one simple operation could lead to a number of kernel launches. Each kernel launch takes time, thus too many launches result in overhead. This motivates us to fuse many kernels into one \"large\" kernel. Another benefit of kernel fusion is to reduce I/O overhead. You can read a very nice [blog post](https://horace.io/brrr_intro.html) by Horace He. We have extracted the main idea below for your reference.\n",
    "\n",
    "We can model the relationship between our memory and compute as factory(compute) and warehouse(memory)\n",
    "![alt text](./assets/kernel_fusion_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, imagine what happens when we perform an unary operation like `torch.cos`. We need to ship our data from our storage to the warehouse, then perform a tiny bit of computation for each piece of data, and then ship that storage back. Shipping things around is quite expensive. As a result, nearly all of our time here is spent shipping data around, and not on the actual computation itself.\n",
    "\n",
    "Since we're spending all of our time on memory-bandwidth, such an operation is called a memory-bound operation, and it means that we're not spending a lot of time on compute.\n",
    "\n",
    "Ok, so that's not ideal. What can we do about it? Let's take a look at how a sequence of operators might look.\n",
    "\n",
    "![alt text](./assets/kernel_fusion_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a very stupid arrangement. Why are we sending the same data to global memory and then back to the compute units, over and over? We should just keep the data at the factory, perform all of our compute, and then send it back! That's actually the spirit of fusion.\n",
    "\n",
    "![alt text](./assets/kernel_fusion_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's see the actual speedup that kernel fusion can bring to us.\n",
    "\n",
    "#### Question 4.3.1\n",
    "\n",
    "Here is the mathematical equation of the GeLU activation function:\n",
    "\n",
    "$$\\text{GELU}(x) \\approx 0.5x\\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right]$$\n",
    "\n",
    "Please implement this function using PyTorch. You can only use torch API for tanh and math operations such as power and square root. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gelu_tanh_python(input):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input (torch.Tensor): Input tensor to apply tanh-approximated GeLU.\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor after GeLU, same shape/dtype/device as input.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 4.3.2\n",
    "Benchmark the latency of your GeLU and PyTorch implemented GeLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Benchmark the latency of your GeLU and PyTorch implemented GeLU. Use torch.nn.functional.gelu(x, approximate=\"tanh\") for the PyTorch GeLU.\n",
    "x = torch.randn(1000000, device=\"cuda\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Python GeLU latency: {python_gelu_latency:.6f} ms\")\n",
    "print(f\"PyTorch GeLU latency: {pytorch_gelu_latency:.6f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 4.3.3\n",
    "\n",
    "Please describe your findings\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 4.3.4\n",
    "\n",
    "Use PyTorch's profiler to explain your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.4 `torch.compile` and CUDA Graph\n",
    "\n",
    "torch.compile is PyTorch's way of speeding up your model by converting eager-mode Python code into optimized kernels. When you wrap a model with torch.compile(model), PyTorch traces the operations, fuses them where possible, and generates efficient Triton or C++ kernels behind the scenes. This reduces Python overhead and combines multiple small operations into fewer, larger kernels—meaning fewer kernel launches and better GPU utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the cell below, we first compile the GeLU function you just implemented and see how your GeLU is optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "compiled_gelu = torch.compile(gelu_tanh_python)\n",
    "\n",
    "x = torch.randn(1000000, device=\"cuda\")\n",
    "\n",
    "with profile(schedule=default_schedule) as prof:\n",
    "    for _ in range(10):\n",
    "        _ = compiled_gelu(x)\n",
    "        prof.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's compile the MLP layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compile the layer\n",
    "mlp = Gemma3MLP(GEMMA_CONFIG).to(device=device, dtype=torch.bfloat16)\n",
    "compiled_mlp = torch.compile(mlp, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 4.4.1\n",
    "\n",
    "Profile the MLP again and describe the difference between the uncompiled version.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 2048\n",
    "hidden_size = GEMMA_CONFIG.hidden_size\n",
    "x = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "raise NotImplementedError()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "torch.compile reduces the number of kernel launches by fusing operations, but the CPU still has to dispatch each remaining kernel individually every forward pass. CUDA graphs go further by recording the entire launch sequence once, then replaying it with a single CPU call. This means the CPU spends almost no time on launch overhead—it just triggers the whole graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mlp = Gemma3MLP(GEMMA_CONFIG).to(device=device, dtype=torch.bfloat16)\n",
    "# compile the layer with CUDA Graphs\n",
    "mlp = torch.compile(mlp, mode=\"max-autotune\")\n",
    "\n",
    "batch_size = 8\n",
    "seq_len = 2048\n",
    "hidden_size = GEMMA_CONFIG.hidden_size\n",
    "x = torch.randn(batch_size, seq_len, hidden_size, device=device, dtype=torch.bfloat16)\n",
    "x = mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bonus 4.4.2\n",
    "\n",
    "Profile the MLP compiled with CUDA again and show the timeline visualization below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 5: Flash Attention\n",
    "\n",
    "In this section, we will learn flash attention and know how it improves the standard attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.1 Limitation of SDPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's see what's the limitation of the standard scale-dot-product attention (SDPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seq_len in [512, 1024, 2048, 4096, 8192, 16384, 32768]:\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    q = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    k = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    v = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    \n",
    "    try:\n",
    "        mem, _ = benchmark_memory(scaled_dot_product_attention, q, k, v, n_warmup=3, n_repeat=10)\n",
    "        attn_matrix_size = 1 * 16 * seq_len * seq_len * 2 / 1e9  # theoretical\n",
    "        print(f\"  seq_len={seq_len:5d}: {mem:.2f} GB (attn matrix: {attn_matrix_size:.2f} GB)\")\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(f\"  seq_len={seq_len:5d}: OOM!\")\n",
    "    \n",
    "    del q, k, v\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 5.1\n",
    "\n",
    "1. What is the memory complexity of standard attention?\n",
    "2. At what sequence length does it become problematic on your GPU?\n",
    "3. What's a major memory overhead of SDPA? And thus, in which LLM scenario could this be a big trouble?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.2 Flash Attention\n",
    "\n",
    "Flash Attention is an algorithm that computes exact self-attention while being both faster and more memory-efficient than the standard implementation. \n",
    "\n",
    "**Standard Attention Approach:**\n",
    "Standard attention computes $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V$ by first materializing the full $N \\times N$ attention matrix $S = QK^T$ in GPU high-bandwidth memory (HBM), then computing softmax row-wise, and finally multiplying by $V$. For long sequences, this $O(N^2)$ memory becomes prohibitive.\n",
    "\n",
    "**Flash Attention Strategy:**\n",
    "Flash Attention avoids materializing the full attention matrix by using tiling and kernel fusion. It divides $Q$, $K$, and $V$ into blocks that fit in GPU SRAM (on-chip fast memory), computes attention block-by-block without storing the full matrix in HBM (off-chip slow memory), and uses online softmax computation to maintain numerical stability across tiles. The key insight is to recompute intermediate values in SRAM rather than loading them from HBM, trading cheap recomputation for expensive memory I/O.\n",
    "\n",
    "**Main improvements:**\n",
    "- Memory reduction from $O(N^2)$ to $O(N)$, enabling longer sequence lengths\n",
    "- 2-4x speedup by minimizing HBM accesses, which are the primary bottleneck on modern GPUs\n",
    "- Exact attention computation with no approximation\n",
    "\n",
    "If you know basic Triton or CUDA programming, we strongly suggest you read the kernel implementation [here](https://github.com/Dao-AILab/flash-attention). Here is the algorithm of flash attention.\n",
    "\n",
    "<img src=\"./assets/flash_attn_algo.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's do some basic benchmarking and compare flash attention with standard attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func\n",
    "\n",
    "results = []\n",
    "\n",
    "for seq_len in [256, 512, 1024, 2048, 4096, 8192]:\n",
    "    # Standard attention uses (B, H, L, D) format\n",
    "    # Flash attention uses (B, L, H, D) format\n",
    "    q_sdpa = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    k_sdpa = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    v_sdpa = torch.randn(1, 16, seq_len, 128, dtype=torch.float16, device=\"cuda\")\n",
    "    \n",
    "    q_flash = q_sdpa.transpose(1, 2).contiguous()  # (B, S, H, D)\n",
    "    k_flash = k_sdpa.transpose(1, 2).contiguous()\n",
    "    v_flash = v_sdpa.transpose(1, 2).contiguous()\n",
    "    \n",
    "    # Benchmark standard\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    sdpa_time = benchmark_latency(scaled_dot_product_attention, q_sdpa, k_sdpa, v_sdpa, n_warmup=3, n_repeat=10)\n",
    "    # std_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    sdpa_mem, _ = benchmark_memory(scaled_dot_product_attention, q_sdpa, k_sdpa, v_sdpa, n_warmup=3, n_repeat=10)\n",
    "    \n",
    "    # Benchmark flash\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    flash_time = benchmark_latency(flash_attn_func, q_flash, k_flash, v_flash, n_warmup=3, n_repeat=10)\n",
    "    # flash_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    flash_mem, _ = benchmark_memory(flash_attn_func, q_flash, k_flash, v_flash, n_warmup=3, n_repeat=10)\n",
    "    \n",
    "    results.append({\n",
    "        \"seq_len\": seq_len,\n",
    "        \"sdpa_ms\": sdpa_time if sdpa_time != float('inf') else None,\n",
    "        \"flash_ms\": flash_time,\n",
    "        \"speedup\": sdpa_time / flash_time if sdpa_time != float('inf') else None,\n",
    "        \"sdpa_mem_gb\": sdpa_mem if sdpa_mem != float('inf') else None,\n",
    "        \"flash_mem_gb\": flash_mem,\n",
    "    })\n",
    "    \n",
    "    del q_sdpa, k_sdpa, v_sdpa, q_flash, k_flash, v_flash\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df_flash = pd.DataFrame(results)\n",
    "print(df_flash.to_string(index=False))\n",
    "plot_flash_attn_improvement(\n",
    "    df = df_flash,\n",
    "    title = \"FlashAttention vs SDPA\",\n",
    "    latency_cols = (\"sdpa_ms\", \"flash_ms\"),\n",
    "    memory_cols = (\"sdpa_mem_gb\", \"flash_mem_gb\"),\n",
    "    seq_len_col = \"seq_len\",\n",
    "    annotate_speedup = True,\n",
    "    annotate_mem_reduction = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use the profiler we just learned to see how flash attention reduces kernels by fusing the attention operation into one joint kernel. (Note that if you encounter negative results, you can restart the notebook kernel and rerun this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.profiler import ProfilerActivity, profile\n",
    "\n",
    "# Directly profiling the flash_attn_func is not stable, so we use the PyTorch implementation of \n",
    "# flash attention which is integrated in torch.nn.functional.scaled_dot_product_attention\n",
    "def torch_flash_attn(q, k, v):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q (torch.Tensor): Query tensor in (B, S, H, D) format for flash attention.\n",
    "        k (torch.Tensor): Key tensor in (B, S, H, D) format.\n",
    "        v (torch.Tensor): Value tensor in (B, S, H, D) format.\n",
    "    Returns:\n",
    "        torch.Tensor: Flash attention output with shape (B, S, H, D).\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "seq_len = 4096\n",
    "q_sdpa = torch.randn(16, 32, seq_len, 128, dtype=torch.bfloat16, device=\"cuda\")\n",
    "k_sdpa = torch.randn(16, 32, seq_len, 128, dtype=torch.bfloat16, device=\"cuda\")\n",
    "v_sdpa = torch.randn(16, 32, seq_len, 128, dtype=torch.bfloat16, device=\"cuda\")\n",
    "q_flash = q_sdpa.transpose(1, 2).contiguous()\n",
    "k_flash = k_sdpa.transpose(1, 2).contiguous()\n",
    "v_flash = v_sdpa.transpose(1, 2).contiguous()\n",
    "\n",
    "# Profile standard\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=default_schedule) as prof_sdpa:\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10): \n",
    "            _ = scaled_dot_product_attention(q_sdpa, k_sdpa, v_sdpa)\n",
    "            prof_sdpa.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Profile flash\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=default_schedule) as prof_flash:\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10): \n",
    "            _ = torch_flash_attn(q_flash, k_flash, v_flash)\n",
    "            prof_flash.step()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "sdpa_kernels = count_cuda_kernels(prof_sdpa)\n",
    "flash_kernels = count_cuda_kernels(prof_flash)\n",
    "\n",
    "print(f\"SDPA: {sdpa_kernels['total']} kernels\")\n",
    "print(f\"Flash Attention: {flash_kernels['total']} kernels\")\n",
    "print(f\"\\nKernel fusion reduces {sdpa_kernels['total'] - flash_kernels['total']} kernel launches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question 5.2\n",
    "\n",
    "Based on our profiling and analysis above, please answer the following questions:\n",
    "\n",
    "1. Why is standard attention memory-bound?\n",
    "2. How does Flash Attention make attention more compute-bound?\n",
    "3. Why does the speedup increase with sequence length?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "1.\n",
    "2.\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Latency, MAC&FLOPs, Memory**: How to calculate theoretical computational requirements\n",
    "2. **Roofline Model**: How to identify compute vs memory bottlenecks\n",
    "3. **Profiling**: How to use PyTorch Profiler to measure actual performance\n",
    "4. **Kernel Optimization**: How kernel fusion, torch.compile, and CUDA Graphs reduce overhead\n",
    "5. **Flash Attention**: How algorithmic improvements can dramatically improve efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
